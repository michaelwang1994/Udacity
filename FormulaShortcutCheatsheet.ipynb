{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "\n",
    "\n",
    "* Population Standard Deviation\n",
    "    * $ \\sigma = \\sqrt{\\dfrac{\\Sigma(x - \\bar{x})}{n}} $\n",
    "* Sample Standard Deviation\n",
    "    * $ s = \\sqrt{\\dfrac{\\Sigma(x - \\bar{x})}{n - 1}} $\n",
    "    * The \"-1\" makes the SD larger, to account for the population SD.\n",
    "* Precision\n",
    "    * $ \\dfrac{\\text{True Positives}}{\\text{True Positives + False Positives}} $\n",
    "* Recall\n",
    "    * $ \\dfrac{\\text{True Positives}}{\\text{True Positives + False Negatives}} $\n",
    "* F1 Score\n",
    "    * $ \\dfrac{2 * (\\text{precision} * \\text{recall})}{(\\text{precision} + \\text{recall})} $\n",
    "    * The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0\n",
    "* Bias/Variance\n",
    "    * Bias is underfitting (oversimplification) and Variance is overfitting (overcomplification)\n",
    "* Entropy\n",
    "    * $ \\sum_{i} -p_i \\log_2(p_i) $\n",
    "    * with $p_i$ being the fraction of examples in class i\n",
    "* Information Gain\n",
    "    * entropy(parent) - [weighted average] * entropy(children)\n",
    "        * Decision trees try to maximize informtion gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
