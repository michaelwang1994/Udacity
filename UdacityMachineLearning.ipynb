{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hechengwang/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "\n",
    "\n",
    "* Population Standard Deviation\n",
    "    * $ \\sigma = \\sqrt{\\dfrac{\\Sigma(x - \\bar{x})}{n}} $\n",
    "* Sample Standard Deviation\n",
    "    * $ s = \\sqrt{\\dfrac{\\Sigma(x - \\bar{x})}{n - 1}} $\n",
    "    * The \"-1\" makes the SD larger, to account for the population SD.\n",
    "* Precision\n",
    "    * $ \\dfrac{\\text{True Positives}}{\\text{True Positives + False Positives}} $\n",
    "* Recall\n",
    "    * $ \\dfrac{\\text{True Positives}}{\\text{True Positives + False Negatives}} $\n",
    "* F1 Score\n",
    "    * $ \\dfrac{2 * (\\text{precision} * \\text{recall})}{(\\text{precision} + \\text{recall})} $\n",
    "    * The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0\n",
    "* Bias/Variance\n",
    "    * Bias is underfitting (oversimplification) and Variance is overfitting (overcomplification)\n",
    "* Entropy\n",
    "    * $ \\sum_{i} -p_i \\log_2(p_i) $\n",
    "    * with $p_i$ being the fraction of examples in class i\n",
    "* Information Gain\n",
    "    * entropy(parent) - [weighted average] * entropy(children)\n",
    "        * Decision trees try to maximize informtion gain\n",
    "* Sigmoid\n",
    "    * $ \\sigma(a) = \\dfrac{1} {1 + e^{-a}} $\n",
    "* Bayes Rule\n",
    "    * $ Pr(h|D) = \\dfrac{Pr(D|h)Pr(h)}{Pr(D)} $\n",
    "        * $ Pr(D) $ is the prior on the data\n",
    "        * $ Pr(D|h) $ is the data given the hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGGlJREFUeJzt3XuYVPV9x/H3FxBRYzRKggEU8RJBiRKMBIKmUzW6YCKY\nakRNGrxUkmLa+DSpJn0sYxubJmmsicZ4R/OIXSMosEYjoG6sIohXJFwVWGG5xCCYSAkuu9/+8Rt0\nsu5lZnd2f3POfF7Pc56dM3MYvs+w+9nD9/x+v2PujoiIpEuP2AWIiEjpKdxFRFJI4S4ikkIKdxGR\nFFK4i4ikkMJdRCSF2g13M7vTzLaY2ZI2jvmZma02s5fNbHhpSxQRkWIVcuY+DTiztRfNbCxwpLsf\nDUwGbilRbSIi0kHthru7Pw1sa+OQ8cAvc8cuAg4ws36lKU9ERDqiFD33AcD6vP363HMiIhKJLqiK\niKRQrxK8Rz1waN7+wNxzH2BmWshGRKQD3N2KOb7QcLfc1pI5wBTgfjMbBWx39y1tFFhMfdKGbDZL\nNpuNXUZq6PNs29tvw+9+B8uXw9q1sGZN+Lp2LWzbBoccErZ+/aC+Psu4cVn69YO+feHAA+GAA8LX\nPVufPmBFxVXlsg58UO2Gu5ndB2SAg83sDWAq0Btwd7/N3R8xs3Fm9hqwA7i46CpEpKxs2QLPPguL\nFsGSJbB0KWzdCsceC0OHwhFHQFUVDB4ctv79oUdekzebDZvE0264u/uFBRxzRWnKEZEYNm2CuXNh\n3jxYsCCciY8aFbbLL4dhw0KI99BVusQoRc9dIslkMrFLSJVK+jzdwxn5/ffDI49AXR2cfjp8/vPw\nve/BkCGdC/JK+izLlXVnD9zMXD13kXjWrYN77oHqati5E84/H8aPh5EjoZdO9cqWmXXZBVURSaim\nJnjsMbj55tBHv/BCmDYNPvMZXdBMM4W7SEo1Noa2y/e/D/vsA1OmhP19941dmXQHhbtIyriHEJ86\nFT76UfjpT0M/XWfplUXhLpIiL74I//AP8H//F9owp56qUK9UGtgkkgI7doRQP+ssmDQJFi+G005T\nsFcyhbtIwv3v/8IJJ8D27bBsGVx2GfTsGbsqiU1tGZGEamqC664L7ZdbbglDGkX2ULiLJNBbb8FX\nvgJ/+hO88EKY/i+ST20ZkYRZvTpMOho6FJ54QsEuLVO4iyTIwoXwuc/BVVfBT34Ce+0VuyIpV2rL\niCTE3Llw0UVw991hVIxIWxTuIgkwb17osc+aBWPGxK5GkkDhLlLm5s8P68E89JCCXQqnVSFFytgL\nL4SbYsycGXrtUpk6siqkLqiKlKm1a+Hss+H22xXsUjyFu0gZ2rYNxo6F734XJkyIXY0kkdoyImWm\nqQm+8AX4xCfghhtiVyPlQG0ZkRT493+Hd96BH/84diWSZBotI1JGHn009Niff14TlKRz1JYRKROb\nN8Pw4TBjBpx8cuxqpJyoLSOSUO5w6aVw+eUKdikNhbtIGbj1VtiyBa65JnYlkhZqy4hE9tprMGoU\nPP00DBkSuxopR2rLiCSMO3zjG3D11Qp2KS2Fu0hE990Hb74J3/pW7EokbdSWEYlk2zY49liYPTvc\nfEOkNR1pyyjcRSL5+tehVy+46abYlUi560i4axKTSASvvBKW8F25MnYlklbquYt0M3f4p3+CqVPh\nwANjVyNppXAX6WaPPAL19WHCkkhXUbiLdKOGBvj2t+G//iv020W6isJdpBvdfTf07w/jxsWuRNJO\no2VEusm778LRR0N1NYweHbsaSRLNUBUpY3fdFca1K9ilOxQU7mZWZWYrzGyVmV3VwusfNrM5Zvay\nmb1qZpNKXqlIgu3aBdddB9deG7sSqRTthruZ9QBuAs4EjgMuMLPmq2BMAX7n7sOBvwZ+Yma6XCSS\nc8cdcMIJmokq3aeQAB4JrHb3OgAzqwbGAyvyjnFg/9zj/YGt7r67lIWKJNWuXfCDH8CsWbErkUpS\nSFtmALA+b39D7rl8NwHHmtlG4BXgH0tTnkjyTZ8Ow4bBpz8duxKpJKVqnZwJvOTup5rZkcA8Mzve\n3d9pfmA2m33vcSaTIZPJlKgEkfLT1BTGtN94Y+xKJElqa2upra3t1Hu0OxTSzEYBWXevyu1fDbi7\n/zDvmIeBH7j7M7n9x4Gr3P35Zu+loZBSUWpqIJsNN7y2ogayibyvq4ZCLgaOMrNBZtYbmAjMaXZM\nHXB6roh+wCeANcUUIpJGP/4xfOc7Cnbpfu22Zdy90cyuAOYSfhnc6e7LzWxyeNlvA74P3G1mS3J/\n7J/d/a0uq1okARYuhPXr4dxzY1cilUgzVEW6yHnnwec+B9/8ZuxKJOl0sw6RMrFhAxx/PNTVwf77\nt3+8SFu0/IBImbjtNrjwQgW7xKMzd5ESe/ddGDQIHn88rCUj0lk6cxcpAw8+CEOHKtglLoW7SIn9\n/OcwZUrsKqTSKdxFSmjJEli7FsaPj12JVDqFu0gJ3XpruDeqbqEnsemCqkiJ7NwJAwfCyy/DoYfG\nrkbSRBdURSJ66CE46SQFu5QHhbtIidx1F1xySewqRAK1ZURKYN26sF57fT3svXfsaiRt1JYRieTu\nu8OMVAW7lAuduYt0UlMTDB4Ms2fD8OGxq5E00pm7SARPPAF9+yrYpbwo3EU66Z57YNKk2FWI/CW1\nZUQ6YccOGDAAVq2Cj30sdjWSVmrLiHSzmhoYPVrBLuVH4S7SCdOnh1EyIuVGbRmRDtq6FY44Itx1\nSTflkK6ktoxIN3rgARg7VsEu5UnhLtJB992nloyUL7VlRDqgrg5OPBE2boTevWNXI2mntoxIN6mu\nhr/5GwW7lC+Fu0gHqCUj5U7hLlKklSvhzTfhlFNiVyLSOoW7SJFmzoQvfQl66KdHypi+PUWKNGMG\nnHde7CpE2qZwFynC66+HETInnxy7EpG2KdxFijBzJpxzDvTsGbsSkbYp3EWKMGMGnHtu7CpE2qdJ\nTCIFqqsL90ndtAl69YpdjVQSTWIS6UIzZ8KECQp2SQaFu0iB1JKRJFFbRqQAGzaEe6Ru2gR77RW7\nGqk0asuIdJEHH4Szz1awS3IUFO5mVmVmK8xslZld1coxGTN7ycyWmtmTpS1TJK6ZM8NCYSJJ0W5b\nxsx6AKuA04CNwGJgoruvyDvmAGABcIa715tZX3f/QwvvpbaMJM4f/gBHHQWbN0OfPrGrkUrUVW2Z\nkcBqd69z9wagGhjf7JgLgZnuXg/QUrCLJNXDD8PppyvYJVkKCfcBwPq8/Q255/J9AjjIzJ40s8Vm\n9tVSFSgS2+zZML756YxImSvViN1ewAjgVGA/4Fkze9bdXyvR+4tEsXMnPPEE3Hln7EpEilNIuNcD\nh+XtD8w9l28D8Ad3/zPwZzN7CjgB+EC4Z7PZ9x5nMhkymUxxFYt0o/nzYcQIOOig2JVIJamtraW2\ntrZT71HIBdWewErCBdVNwHPABe6+PO+YIcCNQBWwN7AION/dlzV7L11QlUS57DIYNgy+9a3YlUgl\n65ILqu7eCFwBzAV+B1S7+3Izm2xml+eOWQE8BiwBFgK3NQ92kaRpbISaGvXbJZk0Q1WkFQsWwNe/\nDkuWxK5EKp1mqIqUkEbJSJIp3EVaoXCXJFO4i7Rg5Up45x048cTYlYh0jMJdpAWzZ4eFwqyoLqdI\n+VC4i7RALRlJOo2WEWlmyxY45pjwde+9Y1cjotEyIiVRUwNnnqlgl2RTuIs0o5aMpIHaMiJ5duyA\nj38c3ngDDjwwdjUigdoyIp00fz6cdJKCXZJP4S6Sp6YGvvjF2FWIdJ7aMiI5TU3Qvz888wwceWTs\nakTep7aMSCcsXhzWbVewSxoo3EVyamrCrFSRNFC4i+So3y5ponAXAerqYONGGDUqdiUipaFwFyGc\ntY8bBz17xq5EpDQU7iKo3y7po6GQUvH++EcYOBDq62H//WNXI/JBGgop0gFz58Lo0Qp2SReFu1Q8\njZKRNFJbRipaYyMccgg8/zwMGhS7GpGWqS0jUqSFC8OSAwp2SRuFu1S0OXPUkpF0UrhLRVO/XdJK\n4S4V6/XX4a23wvrtImmjcJeKVVMDX/gC9NBPgaSQvq2lYqklI2mmoZBSkbZvh8MOg02bYL/9Ylcj\n0jYNhRQp0G9+A6ecomCX9FK4S0XSQmGSdmrLSMVpaAizUpcsgQEDYlcj0j61ZUQK8MwzcPjhCnZJ\nN4W7VByNkpFKoHCXiuKuJQekMijcpaIsWwa7dsGIEbErEelaBYW7mVWZ2QozW2VmV7Vx3Elm1mBm\nXypdiSKlM2sWTJgAVtSlKZHkaTfczawHcBNwJnAccIGZDWnluP8EHit1kSKlMmsWnHNO7CpEul4h\nZ+4jgdXuXufuDUA1ML6F474JzAB+X8L6REpm/XpYsyZMXhJJu0LCfQCwPm9/Q+6595hZf2CCu/8C\n0H94pSzNnh0WCuvVK3YlIl2vVBdUbwDye/EKeCk7aslIJSnkHKYeOCxvf2DuuXyfBqrNzIC+wFgz\na3D3Oc3fLJvNvvc4k8mQyWSKLFmkeNu2wXPPwRlnxK5EpH21tbXU1tZ26j3aXX7AzHoCK4HTgE3A\nc8AF7r68leOnATXu/mALr2n5AYni3nthxoxw9i6SNB1ZfqDdM3d3bzSzK4C5hDbOne6+3Mwmh5f9\ntuZ/pJgCRLrDniGQIpVCC4dJ6u3cGRYKW7MGDj44djUixdPCYSItmD8/zEhVsEslUbhL6qklI5VI\nbRlJtd27oX//MFLm8MNjVyPSMWrLiDTz29/CoEEKdqk8CndJtQcegPPOi12FSPdTW0ZSa/fucLel\nhQth8ODY1Yh0nNoyInmeegoOPVTBLpVJ4S6p9atfqSUjlUttGUmlPS2ZZ5+FI46IXY1I56gtI5Lz\n1FMwcKCCXSqXwl1SSaNkpNKpLSOps6cls2ABHHlk7GpEOk9tGRFCS2bAAAW7VDaFu6ROdTV8+cux\nqxCJS20ZSZVdu8JaMi+9BIcd1v7xIkmgtoxUvEcfhWHDFOwiCndJlenT4aKLYlchEp/aMpIab78d\nztjXroWDDopdjUjpqC0jFW3mTDj1VAW7CCjcJUXUkhF5n9oykgr19fDJT8LGjdCnT+xqREpLbRmp\nWP/zP+E+qQp2kUDhLonnDtOmwcUXx65EpHwo3CXxFi2ChgY4+eTYlYiUD4W7JN5dd8Ell4AV1ZEU\nSTddUJVE27Ej3Epv6dKw7IBIGumCqlScGTPgs59VsIs0p3CXRLvrLrj00thViJQftWUksV57DcaM\ngfXroXfv2NWIdB21ZaSi3Hor/O3fKthFWqIzd0mknTvDImGLFukm2JJ+OnOXilFdDSNHKthFWqNw\nl0S6+WaYMiV2FSLlS+EuifPcc7B1K1RVxa5EpHwp3CVxfv5z+MY3oIe+e0VapQuqkihbtsCQIWEY\n5MEHx65GpHt02QVVM6sysxVmtsrMrmrh9QvN7JXc9rSZfbKYIkQKddNNMHGigl2kPe2euZtZD2AV\ncBqwEVgMTHT3FXnHjAKWu/vbZlYFZN19VAvvpTN36bAdO+Dww2HBAjj66NjViHSfrjpzHwmsdvc6\nd28AqoHx+Qe4+0J3fzu3uxAYUEwRIoWYNg1OOUXBLlKIXgUcMwBYn7e/gRD4rbkMeLQzRYk0t3s3\nXH99uE+qiLSvkHAvmJn9NXAx0OptE7LZ7HuPM5kMmUymlCVISj34YFj5cfTo2JWIdL3a2lpqa2s7\n9R6F9NxHEXroVbn9qwF39x82O+54YCZQ5e6vt/Je6rlL0dzhxBNh6lQYP77940XSpqt67ouBo8xs\nkJn1BiYCc5r9xYcRgv2rrQW7SEfNmRMC/uyzY1cikhzttmXcvdHMrgDmEn4Z3Onuy81scnjZbwOu\nAQ4CbjYzAxrcva2+vEhB3CGbDWftuo2eSOE0iUnK2qxZcO218OKLCnepXFoVUlLFPQR7NqtgFymW\nwl3K1owZYf0Y9dpFiqe2jJSlXbvg2GPh9tvh1FNjVyMSl9oykhq/+EVYIEzBLtIxOnOXsrNtGxxz\nDDz5JBx3XOxqROLryJm7wl3Kzre/DX/6U7gBtoh0LNxLuvyASGctXQq//CW8+mrsSkSSTT13KRtN\nTeEOS9deC/36xa5GJNkU7lI27rknjJK5/PLYlYgkn3ruUha2bg0XTx95BEaMiF2NSHnRBVVJrIkT\nw5K+118fuxKR8qMLqpJI998Pr7wS7rQkIqWhM3eJatMmGD4cHn4YTjopdjUi5UkzVCVRGhvha1+D\nyZMV7CKlpnCXaK67LoyO+dd/jV2JSPqo5y5RzJ8Pt9wCL7wAvfRdKFJy+rGSbrd2LXz1q3DvvfDx\nj8euRiSd1JaRbrV9O5x1Fnzve3DaabGrEUkvjZaRbtPQEIL9mGPgxhtjVyOSHJrEJGWrqQkmTQoz\nUWfPVp9dpBiaxCRlyT0sCFZXB48+qmAX6Q76MZMu5Q5XXglLlsDcubDvvrErEqkMCnfpMrt3hwlK\nS5fCY4/B/vvHrkikcijcpUvs3AkXXBC+Pv44fOhDsSsSqSwaCiklt2EDZDKhBVNTo2AXiUHhLiX1\n9NMwciSccw5Mnw69e8euSKQyqS0jJdHYCD/6EdxwQ7ijUlVV7IpEKpvCXTptzZqwnECfPvD883Do\nobErEhG1ZaTDdu2C//iPsFzvuefCvHkKdpFyoTN3KZp7uNfplVfC0KHhbH3w4NhViUg+hbsUzD0M\na7zmGvjjH+G//zusFSMi5UfhLu1qaIBZs8LF0jffhGwWzj8fevaMXZmItEbhLq2qqwtrrt9yS2i7\nXHklTJigtWFEkkA/pvIXfv/7cJZ+772wbFm4UDpnDnzqU7ErE5FiaMnfCtfYCC++GC6Q/vrXsGoV\nnHEGXHQRjB2rSUgi5aDL1nM3syrgBsLQyTvd/YctHPMzYCywA5jk7i+3cIzCPbLt28PolmeeCdui\nRTBwYAjycePg5JMV6CLlpkvC3cx6AKuA04CNwGJgoruvyDtmLHCFu59lZp8Bfuruo1p4L4V7CdXW\n1pLJZD7wvHu4KcbatbB6Nbz6atiWLIFt22D4cBgzJmyjR0Pfvt1fezlq7fOU4umzLK2uulnHSGC1\nu9fl/pJqYDywIu+Y8cAvAdx9kZkdYGb93H1LMcVI+xobw4iVzZvhjjtqWbcuw+bNYf+NN8Js0TVr\noEcPOPLIsA0bBpddBscfD4cfHl6TD1IglY4+y/gKCfcBwPq8/Q2EwG/rmPrccxUR7u7hNnKNjWEN\n8+bbu++GpW/3bH/+c+v7O3bA22+H9smer/mPd+yAgw6CQw6Bd94JwxH79QszQ8eMgSOOCNtHPhL7\nUxGRmLp9tExVVQjDPd2ZPY9b2i+HY5qaWg7s5hvAXnuFYYLNt732gn32+cutT58PPrfPPrDffnDU\nUXDggXDAAe9/3fP4wx9+f3x5Nhs2EZHmCum5jwKy7l6V278a8PyLqmZ2C/Cku9+f218B/FXztoyZ\nqeEuItIBXdFzXwwcZWaDgE3AROCCZsfMAaYA9+d+GWxvqd9ebHEiItIx7Ya7uzea2RXAXN4fCrnc\nzCaHl/02d3/EzMaZ2WuEoZAXd23ZIiLSlm6dxCQiIt2jWwbFmdm5ZrbUzBrNbESz175rZqvNbLmZ\nndEd9aSJmU01sw1m9mJu0z2QimRmVWa2wsxWmdlVsetJOjNbZ2avmNlLZvZc7HqSxszuNLMtZrYk\n77mPmNlcM1tpZo+Z2QHtvU93jXh+FTgH+G3+k2Y2FPgyMJQwu/VmM1NfvnjXu/uI3Pab2MUkSW6S\n3k3AmcBxwAVmNiRuVYnXBGTc/VPu3nzYtLRvGuH7Md/VwHx3PwZ4Avhue2/SLeHu7ivdfTXQPLjH\nA9Xuvtvd1wGr+eAYemmffiF23HuT9Ny9AdgzSU86ztBd3jrM3Z8GtjV7ejxwT+7xPcCE9t4n9j9A\na5OfpDhXmNnLZnZHIf9dk7/Q0iQ9fQ92jgPzzGyxmf1d7GJS4mN7RiC6+2bgY+39gZJNYjKzeUC/\n/KcI/8j/4u41pfp7KlFbny1wM/Bv7u5m9n3geuDS7q9S5D1j3H2TmX2UEPLLc2ejUjrtjoQpWbi7\n++c78MfqgfxbKg/MPSd5ivhsbwf0i7Q49cBhefv6Huwkd9+U+/qmmT1EaH0p3Dtny571uszsEOD3\n7f2BGG2Z/P7wHGCimfU2s8HAUYCurhch9w+9x5eApbFqSaj3JumZWW/CJL05kWtKLDPb18w+lHu8\nH3AG+p7sCOODWTkp9/hrwOz23qBb1pYxswnAjUBf4GEze9ndx7r7MjP7FbAMaAD+XmsCF+1HZjac\nMEJhHTA5bjnJ0tokvchlJVk/4KHcUiO9gOnuPjdyTYliZvcBGeBgM3sDmAr8J/CAmV0C1BFGGbb9\nPspSEZH0iT1aRkREuoDCXUQkhRTuIiIppHAXEUkhhbuISAop3EVEUkjhLiKSQgp3EZEU+n+qFwSp\nrieewgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10404a7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.arange(-10, 10, 0.01)\n",
    "plt.plot(a, 1/(1 + np.exp(-a)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "* Neural Networks\n",
    "    * $ \\sum_{i = 1}^K x_iw_i \\ge \\theta $ with the first term being the activation and the second tem being the firing threshold.\n",
    "    * Perceptron Rule\n",
    "        * For each iteration of the Neural Network, we'd want to change our weights if:\n",
    "            * $ w_i = w_i + \\Delta w_i $\n",
    "                * $ \\Delta w_i = \\mathcal{N}(y - \\hat{y})x_i $ for $\\mathcal{N}$ the learning rate and $ x$ the input\n",
    "                    * $ \\hat{y} = (\\sum_i w_i x_i \\ge 0) $ for $ \\hat{y} $ the prediction and $ y $ the target\n",
    "                        * i.e. no change if true, change if false\n",
    "                        * **Only useful for linearly separable data **\n",
    "    * Gradient Descent (min least squares)\n",
    "        * $ a = \\sum_i x_i w_i $ and $ \\hat{y} = \\{a \\ge 0\\} $\n",
    "        * $ e(w) = \\frac{1}{2} \\sum_{(x,y) \\in D)}(y - a)^2 $\n",
    "        * $ \\frac{\\delta e}{\\delta w_i} = \\frac{\\delta}{\\delta w_i} \\frac{1}{2} \\sum_{(x,y) \\in D)}(y - a)^2$\n",
    "        * $ = \\sum_{(x,y) \\in D)} (y-a) \\frac{\\delta}{\\delta w_i} - \\sum_i x_iw_i'$\n",
    "        * $ = \\sum_{(x,y) \\in D)} (y-a)(-x_i) $\n",
    "            * **Robust for non-linearly separable data too**\n",
    "    * Restrication Bias\n",
    "        * Representational Power: we can make perceptions, sigmoids, boolean functions, continuous functions, arbitrary complicated neural networks.\n",
    "    * Preference Bias\n",
    "        * Start with small random values\n",
    "* SVM\n",
    "    * Given the formula for two planes are $ w^Tx_1 + b = 1, w^Tx_2 + b = -1$, we want to maximize their distance:\n",
    "        * $ \\max(x_1 - x_2) = \\max \\dfrac{2}{||w||} $ while classifying everything correctly $ y_i (w^Tx_i + b) \\ge 1 $\n",
    "        * An easier solution is $ min \\frac{1}{2} ||w|| ^ 2 $\n",
    "            * $ W(a) = \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{ij} \\alpha_i \\alpha_j y_i y_j x_i^T x_j $\n",
    "                * $ x_i^T x_j $ is a measure of the similarity of x's. \n",
    "            * $ w = \\sum_i \\alpha_i y_i x_i $\n",
    "                * Most $ \\alpha_i = 0 $. So we make a **machine** that finds the **support vectors**.\n",
    "                * Intuitively, only the points near the decision boundary are the ones that matter.\n",
    "    * In the case that points are not linearly separable, we use the **kernel trick**:\n",
    "        * $ \\Phi(q) = < q_1^2, q_2^2, \\sqrt2q_1q_2 > $.\n",
    "        * $ \\Phi(x)^T \\Phi(y) = x_1y_1^2 + 2x_1y_1x_2y_2 + x_2y_2^2 $\n",
    "            * Note that the kernel can be anything we want\n",
    "            * Therefore, for any **distance calculating** kernal $K$, $W(\\alpha) = \\alpha_i - \\frac{1}{2} \\sum_{ij} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) $\n",
    "* Naive Bayes\n",
    "    * Goal is to maximize:\n",
    "        * $h_{MAP} = argmax_h Pr(h|D) $ for maximum a posteriori\n",
    "        * $h_{ML} = argmax_h Pr(D|h) $ for maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
